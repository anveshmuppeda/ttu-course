# Project PCA  

1. Intro  
    1. 1dpca  
    2. 2dpca    // Manasa   
2. Algorithms explaination & psedo codes  // Anvesh  
3. Flow charts and datasets     // Vinnu  
4. 1dpca execution          // Sarat   
5. 2dpca execution          // Revanth  
6. group extraction & Conclution    // Abhishek    



Increased Dimensionality Reduction: 
This means that more variance in the data is captured by the principal components, potentially leading to a better representation of the data.

Improved Discriminative Power: 
This means reduced-dimensional space obtained through PCA may better discriminate between different classes or categories in the data.

Increased Computational Complexity: 
As N increases, the computational complexity of PCA also increases. PCA involves computations such as eigenvalue decomposition or singular value decomposition (SVD), which become more computationally intensive as the dimensionality of the data increases.

Feature Overfitting: 
Overfitting occurs when the model learns to capture noise or idiosyncrasies specific to the training data, leading to poor generalization performance on unseen data.

Diminishing Returns: 
Increasing N beyond a certain point may result in diminishing returns in terms of accuracy improvement. 